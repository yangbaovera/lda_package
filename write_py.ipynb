{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PasteScript\n",
      "Reading https://pypi.python.org/simple/PasteScript/\n",
      "Downloading https://files.pythonhosted.org/packages/e5/f0/78e766c3dcc61a4f3a6f71dd8c95168ae9c7a31722b5663d19c1fdf62cb6/PasteScript-2.0.2.tar.gz#sha256=c03f249805538cc2328741ae8d262a9200ae1c993119b3d9bac4cd422cb476c0\n",
      "Best match: PasteScript 2.0.2\n",
      "Processing PasteScript-2.0.2.tar.gz\n",
      "Writing /tmp/easy_install-r542d6by/PasteScript-2.0.2/setup.cfg\n",
      "Running PasteScript-2.0.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-r542d6by/PasteScript-2.0.2/egg-dist-tmp-v0hgg3n0\n",
      "Warning: no news for this version found\n",
      "warning: no previously-included files matching '*' found under directory 'docs/_build/_sources'\n",
      "warning: no files found matching '*.js' under directory 'paste'\n",
      "warning: no files found matching '*.jpg' under directory 'paste'\n",
      "creating /opt/conda/lib/python3.6/site-packages/PasteScript-2.0.2-py3.6.egg\n",
      "Extracting PasteScript-2.0.2-py3.6.egg to /opt/conda/lib/python3.6/site-packages\n",
      "Adding PasteScript 2.0.2 to easy-install.pth file\n",
      "error: [Errno 13] Permission denied: '/opt/conda/lib/python3.6/site-packages/easy-install.pth'\n"
     ]
    }
   ],
   "source": [
    "! easy_install PasteScript  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lda_package/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%file lda_package/setup.py\n",
    "from setuptools import setup\n",
    "\n",
    "setup(name = \"mylda_package\",\n",
    "      version = \"1.0\",\n",
    "      author='Yang Bao, Wenlin Wu',\n",
    "      url='#####',\n",
    "      description='Implementation of Latent Dirichlet Allocation',\n",
    "      packages=setuptools.find_packages()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lda_package/mylda/LDA.py\n"
     ]
    }
   ],
   "source": [
    "%%file lda_package/mylda/LDA.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma\n",
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma\n",
    "\n",
    "# convergence function\n",
    "def is_convergence1(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(list(map(np.sum,np.square(old - new))))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "def is_convergence2(old, new, tol = 10**(-2)):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    TRUR or FALSE\n",
    "    \"\"\"\n",
    "    loss = np.sqrt(np.sum(np.square(old - new)))\n",
    "    return np.max(loss) <= tol\n",
    "\n",
    "def optimize_vp(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        gamma_old = gamma\n",
    "        #update phi\n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                for i in range(K):\n",
    "                    phi[m][n,i] = beta[i,np.int(words[m][n])] * np.exp(digamma(gamma[m,i]))\n",
    "                #nomalize to 1)\n",
    "                phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "        phi_new = phi\n",
    "        #update gamma\n",
    "        for i in range(M):\n",
    "            gamma[i,:]  = alpha + np.sum(phi[i], axis = 0)\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1(phi_old, phi_new) == True and is_convergence2(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma\n",
    "\n",
    "# estimate alpha\n",
    "def alpha_estimate(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # compute the gradient vector and the diagonal part of the Hessian matrix\n",
    "        g = np.zeros(K)\n",
    "        h = np.zeros(K)\n",
    "        for i in range(K):\n",
    "            g1 = M*(digamma(np.sum(alpha))-digamma(alpha[i]))\n",
    "            g2 = 0\n",
    "            for d in range(M):\n",
    "                g2 += digamma(gamma[d,i])-digamma(np.sum(gamma[d,:]))\n",
    "            g[i] = g1 + g2\n",
    "            \n",
    "            h[i] = -M*polygamma(1, alpha[i])\n",
    "        \n",
    "        # compute the constant part\n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "# estimate beta\n",
    "def beta_estimate(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one\n",
    "    for i in range(K):\n",
    "        beta[i,:] = beta[i,:]/sum(beta[i,:])\n",
    "        \n",
    "    return beta\n",
    "\n",
    "# Variation EM\n",
    "def variation_EM(M, K, D, N, V_words, alpha_initial, beta_initial, gamma_initial, phi_initial, iteration = 1000):\n",
    "    \n",
    "    phi_gamma = optimize_vp(phi_initial, gamma_initial, alpha_initial, beta_initial, w_struct, M, N, K)\n",
    "    phi = phi_gamma[0]\n",
    "    gamma = phi_gamma[1]\n",
    "    \n",
    "     \n",
    "    (alpha, beta) = (alpha_initial, beta_initial)\n",
    "    \n",
    "    for t in range(iteration):\n",
    "        \n",
    "        (phi_old, gamma_old) = (phi, gamma)\n",
    "        \n",
    "        alpha = alpha_estimate(gamma, alpha, K, M)\n",
    "        beta = beta_estimate(K, V_words, phi, D)\n",
    "        \n",
    "        phi_gamma1 = optimize_vp(phi, gamma, alpha, beta, D, M, N, K)\n",
    "        phi = phi_gamma1[0]\n",
    "        gamma = phi_gamma1[1]\n",
    "        \n",
    "        if is_convergence2(gamma_old, gamma) and is_convergence1(phi_old, phi):\n",
    "            break\n",
    "    \n",
    "    return alpha, beta, gamma, phi\n",
    "\n",
    "# a new function to calculate log of sum\n",
    "def log_sum(log_a, log_b):\n",
    "    \"\"\"\n",
    "    input: log(a), log(b)\n",
    "    output: log(a+b)\n",
    "    \"\"\"\n",
    "    return log_a + np.log(1+np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "\n",
    "def optimize_vp_opt(phi, gamma, alpha, beta, words, M, N, K, max_iter=500):\n",
    "    '''\n",
    "    optimize the variational parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phi:   ndarray\n",
    "           An array of topic-word matrix\n",
    "    gamma: ndarray\n",
    "           A matrix of doc-topic\n",
    "    alpha: ndarray\n",
    "           the parameter of doc-topic dirichlet distribution\n",
    "    beta:  ndarray\n",
    "           the parameter of topic-word dirichlet distribution\n",
    "    words: list \n",
    "           the list of lists of words in all \n",
    "    M : int, the number of documents\n",
    "    N : ndarraay, the number of words in each document\n",
    "    K : int, the number of topics in the corpus\n",
    "    Returns\n",
    "    -------\n",
    "    out : list of ndarray\n",
    "          the optimized and normalized(sum to 1) phi \n",
    "    '''\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        phi_old = phi\n",
    "        \n",
    "        # we use log(phi) here and following processes\n",
    "        log_phi = np.array(list(map(np.log, phi)))\n",
    "        gamma_old = gamma\n",
    "       \n",
    "        for m in range(M):\n",
    "            for n in range(N[m]):\n",
    "                \n",
    "                logsum = 0\n",
    "                for i in range(K):\n",
    "                    \n",
    "                    # use new method in log form to update phi\n",
    "                    log_phi[m][n,i] = np.log(beta[i,np.int(words[m][n])]) + digamma(gamma[m,i])\n",
    "                    \n",
    "                    logsum = log_sum(logsum, log_phi[m][n,i])\n",
    "                # use new metohd to implement nomalization\n",
    "                log_phi_mn = log_phi[m][n,:] - logsum\n",
    "                log_phi[m][n,:] = log_phi_mn\n",
    "                \n",
    "                phi[m][n,:] = np.exp(log_phi_mn)\n",
    "        \n",
    "            # instead of alpha, use old phi and new phi to iterative\n",
    "            d_phi = phi[m] - phi_old[m]\n",
    "            gamma[m,:]  = gamma[m,:] + np.sum(d_phi, axis = 0)\n",
    "            \n",
    "        phi_new = phi\n",
    "        gamma_new = gamma\n",
    "        \n",
    "        if is_convergence1(phi_old, phi_new) == True and is_convergence2(gamma_old, gamma_new) == True:\n",
    "            break\n",
    "   \n",
    "    return phi, gamma\n",
    "\n",
    "\n",
    "# estimate alpha\n",
    "def alpha_estimate_opt(gamma, alpha_initial, K, M, max_iter = 100):\n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm.\n",
    "    digamma function and polygamma function are used in the following process.\n",
    "    \n",
    "    input:\n",
    "    alpha_initial: the initial setting of alpha, it is an 1*K vector\n",
    "    K: the number of topics\n",
    "    M: the number of documents\n",
    "    gamma: the result from another update function (see gamma_update())\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = alpha_initial\n",
    "    for t in range(max_iter):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        # we use vector instead of calculating in loop\n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) \n",
    "        + np.sum(digamma(gamma) -np.tile(digamma(np.sum(gamma,axis=1)),(K,1)).T,axis=0)\n",
    "        h = -M*polygamma(1,alpha)\n",
    "        \n",
    "        z = M*polygamma(1, np.sum(alpha))\n",
    "        c = (np.sum(g/h))/(z**(-1) + np.sum(h**(-1)))\n",
    "                           \n",
    "        # update alpha                   \n",
    "        alpha -= (g-c)/h\n",
    "        \n",
    "        if is_convergence2(alpha_old, alpha):\n",
    "            break\n",
    "            \n",
    "    return alpha\n",
    "\n",
    "# estimate beta\n",
    "def beta_estimate_opt(K, V_words, phi, D):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is an estimation function, especially used in the process of LDA algorithm\n",
    "    \n",
    "    input:\n",
    "    K: the number of topics\n",
    "    V_words: a vector of all unique words in the vocabulary\n",
    "    D: D = (w_1,w_2,...w_M), contains all words in all documents\n",
    "    phi: the result from another update function (see phi_update())\n",
    "    \n",
    "    output:\n",
    "    beta: the estimate parameter for LDA, it is a K*V matrix\n",
    "    \"\"\"\n",
    "    V = len(V_words)\n",
    "    beta = np.ones((K,V))\n",
    "    # first obtain the propotion values\n",
    "    for j in range(V):\n",
    "        word = V_words[j]\n",
    "        # give a TRUE or FALSE \"matrix\", remember w_mnj should have the same shape with phi\n",
    "        w_mnj = [np.repeat(w==word, K).reshape((len(w),K)) for w in D]\n",
    "        # compute the inner sum over number of words\n",
    "        sum1 = list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))\n",
    "        # compute the outer sum over documents\n",
    "        beta[:,j] = np.sum(np.array(sum1), axis = 0)\n",
    "    \n",
    "    # then normalize each row s.t. the row sum is one, in vector method\n",
    "    beta= beta/ np.sum(beta, axis = 1).reshape((-1,1))\n",
    "        \n",
    "    return beta\n",
    "\n",
    "\n",
    "# Optimize variation EM\n",
    "def variation_EM_new(M, K, D, N, V_words, alpha_initial, beta_initial, gamma_initial, phi_initial, iteration = 1000):\n",
    "    \n",
    "    phi_gamma = optimize_vp_opt(phi_initial, gamma_initial, alpha_initial, beta_initial, w_struct, M, N, K)\n",
    "    phi = phi_gamma[0]\n",
    "    gamma = phi_gamma[1]\n",
    "    \n",
    "     \n",
    "    (alpha, beta) = (alpha_initial, beta_initial)\n",
    "    \n",
    "    for t in range(iteration):\n",
    "        \n",
    "        (phi_old, gamma_old) = (phi, gamma)\n",
    "        \n",
    "        alpha = alpha_estimate_opt(gamma, alpha, K, M)\n",
    "        beta = beta_estimate_opt(K, V_words, phi, D)\n",
    "        \n",
    "        phi_gamma1 = optimize_vp_opt(phi, gamma, alpha, beta, D, M, N, K)\n",
    "        phi = phi_gamma1[0]\n",
    "        gamma = phi_gamma1[1]\n",
    "        \n",
    "        if is_convergence2(gamma_old, gamma) and is_convergence1(phi_old, phi):\n",
    "            break\n",
    "    \n",
    "    return alpha, beta, gamma, phi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lda_package/mylda/_int_.py\n"
     ]
    }
   ],
   "source": [
    "%%file lda_package/mylda/_int_.py\n",
    "\n",
    "from .LDA import is_convergence1, is_convergence2, optimize_vp\n",
    "from .LDA import alpha_estimate, alpha_estimate_opt\n",
    "from .LDA import beta_estimate, beta_estimate_opt\n",
    "from .LDA import log_sum, optimize_vp_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
